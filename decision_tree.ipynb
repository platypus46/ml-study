{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의사결정 트리(Decision Tree) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree는 분류와 회귀가 모두 가능한 지도 학습 모델이며 이름에서 알 수 있 듯이 tree구조에 기반하여 결정을 진행한다는 특징을 갖고 있다.  \n",
    "이는 우리가 하나의 결정을 내리는 자연적인 프로세스와 상충한다고 할 수 있다.    \n",
    "\n",
    "**(쉽게 스무고개를 떠올리면 이해가 쉽다.)**  \n",
    "\n",
    "의사결정트리에 따라 데이터를 구분하기위해서 특정 기준을 만들고 선택하게되는데 이때, 서로 다른 클래스가 많이 섞여있게 기준을 잡고 분할한다면 이를 순도(purity)가 낮다고 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정보 엔트로피(information entrophy)\n",
    "정포 엔트로피는 집합의 순도를 측정하는데 가장 자주 사용되는 지표이다. 엔트로피의 값이 작아질수록 샘플집합의 순도는 높아진다.  \n",
    "샘플 집합을 $D$라고 하고 $D$의 k번째 클래스 샘플이 차지하는 비율을 $p_k$(k=1,2,...,m)라고 한다면 $D$의 엔트로피를 아래와 같이 정의할 수 있게된다.\n",
    "\n",
    "$Ent(D)=-\\displaystyle\\sum_{k=1}^{m} p_klog_2(p_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정보이득(information gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정보이득은 (주어진상황에서의 엔트로피-분기 후 하나의 속성을 잡았을 때의 엔트로피)로 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이산 속성을 가지는 속성 a가 취하는 값이 V개가 있다고 가정한면 식을 아래와 같이 표현할 수 있다.  \n",
    "$Gain(D,a)=Ent(D)-\\displaystyle\\sum_{v=1}^{V}\\frac{|D^v|}{|D|}Ent(D^v)$  \n",
    "일반적으로 정보 이득이 크면 속성a를 사용하여 분할할 때 얻을 수 있는 순도 상승도가 높아지는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정보이득만 따지고 분할하게 된다면 각각의 노드들의 순도는 최고치에 도달할뿐더러 일반화 성능이 매우 저조하여 새로운 샘플에 대해서 유효한 예측을 하기 어려워질 것이다.\n",
    "<br/>  \n",
    "\n",
    "**그럼....분할점은 어떻게 결정하지??**\n",
    "\n",
    "사실 분할방법이야 찾아보면 많겠지만 **C4.5 의사결정 트리**에서는 <u>정보 이득율(information gain ratio)</u>을 사용하고 **CART 의사결정 트리**에서는 <u>지니계수(gini index)</u>를 사용하여 최적의 분할 속성을 선택하는 방법을 제시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정보 이득율(information gaion ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정보 이득율의 식은 다음과 같이 표현된다.  \n",
    "**Gain_ratio(D,a)=$\\frac{Gain(D,a)}{IV(a}$**  \n",
    "$IV(a)=-\\displaystyle\\sum_{v=1}^{V}\\frac{|D^v|}{|D|}log_2\\frac{|D^v|}{|D|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 알아야두어야 할 점은 이득율 규칙은 취할 수 있는 값의 수가 비교적 적은 속성에 편향되어있기 때문에 이득율이 가장 크다고 분할 속성을 선택하면 안된다라는 것이다.  \n",
    "\n",
    "그렇기에 휴리스틱(heuristic)을 사용하여 후보군 중 <u>정보 이득이 평균 수준보다 높은 속성을 찾아내 그 중 이득율이 가장 높은 것을 선택</u>한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지니계수(gini index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 식은 지니값을 나타낸 것이다.  \n",
    "$Gini(D)=\\displaystyle\\sum_{k=1}^{m}\\displaystyle\\sum_{k'\\ne k}^{}p_k p_{k'}=1-\\displaystyle\\sum_{k=1}^{m}p_k^2$  \n",
    "Gini(D)는 D에서 임의로 고른 두개의 샘플이 서로 다른 클래스일 확률을 나타낸다.  \n",
    "따라서 Gini(D)가 작을수록 데이터 세트의 순도는 높다.\n",
    "\n",
    "속성 a의 지니계수는 다음과 같이 정의할 수 있다.  \n",
    "**Gini_index(D,a)=$\\displaystyle\\sum_{v=1}^{V}\\frac{|D^v|}{|D|}Gini(D^v)$**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
